---
title: "Radicalization: Certain Remarks"
author: Damon Stanley
date: September 29, 2021
bibliography: blogs/2021-09-29-radicalization.bib
link-citations: false
...

# 2011

> The water played against the grass in the reflection off the boat's white prow. A bicycle rested against the concrete dock, its wheel not yet coming to rest, as the intercoastal gently rose and fell, wending its way along an alley of private docks and shaded mansions. It had taken only a minute for the officer to arrive and a quick, clipped conversation to send me off: it was not my dock; I had no right to be there. The sun glistened on the water, casting vague shadows on proprietary beaches. One was still permitted, at least, to look.

"What radicalized you?" Spend any time online in lefty (or, at least I imagine, righty) circles and you will see this question asked and answered. [This tweet](https://twitter.com/afrodykee/status/1443570807293284353) is my recent favorite, unhelpfully but unimpeachably pointing out that accepting radical claims is a step in the direction of accepting radical claims.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I think what radicalized me is realizing that it’s either Black liberation or death.</p>&mdash; 💎 (legally) blind himbo sailor mars 🦯 💎 (\@afrodykee) <a href="https://twitter.com/afrodykee/status/1443570807293284353?ref_src=twsrc%5Etfw">September 30, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

These narratives posit radicalization as a flash of insight, a sudden transformation from which there is no return. Hence the metaphor used by online right-wing fora: taking the red pill (which, in *The Matrix*, wakes one from a false but pleasant virtual reality into a harsh light). Academics studying radicalization, though, conceptualize it as a staged process, a staircase one can ascend more or less completely [@moghaddamStaircaseTerrorismPsychological2005]. After all, extreme views are not typically given in a single formula, but a patchwork of claims one can accept or reject piecemeal.

# 2012

> Placing down the cell phone, the pen found itself raised hesitantly before the ballot. Light filtered messily into the messy dorm room, dust roiling in lukewarm air. My mother had just finished describing the campaign's last push, an excursion into our hometown's Haitian neighborhood, a door-to-door effort to secure Obama's victory in our gerrymandered district. A state away, voting by mail and for the first time, the pen's tip considered various presidential circles. In a moment it found and blacked in the socialist candidate. I cannot remember their name.

Children, Judith Harris notoriously suggested, are not formed by parenting. They are socialized in the first instance by their peers [@harrisNurtureAssumptionWhy2009]; children sand away edges to their personality to ingratiate themselves to their peers, edges themselves developed by the child's status-driven need to differentiate themselves from those selfsame peers [@harrisNoTwoAlike2010]. Such differentiation acts on small, largely genetically based, variations in the child (this one is a bit larger than their peers, this one a bit more naturally outgoing, and so on). The theory thus predicts that the child's personality is formed in largest part in the womb and on the playground and not in the home. In the public consciousness, the moral implications of Harris' theory weighed far more heavily than its scientific foundation. If sloppy parenting is not what made Suzie Q the little terror that she is, then her beleaguered parents might at least accept the consolation that it was not altogether their fault. On the flip side, the proud parents of wunderkind Lonnie Z ought instead to be blessing their lucky stars rather than their tedious implementation of every letter of a bookshelf of parental advice. 

A child's character, their concomitant successes and failures, and the parent's responsibility therefor form poles in a generational drama. The same can be said for a child's political commitments. A child's political divergence from their parents can be the source of a schism between them, or at least awkwardness at the Thanksgiving table. In the reverse direction, this political divergence can instead be understood as an expression of that schism. As Robert Lindner described a patient's fascism, "[fascism gave him] a whole world to hate, in extension of his primary hatred of the father; targets on which to exercise brutality and revenge, both as expressions of identity with the father’s strength and dominance, and in retaliation for the hurts of childhood" [@lindnerFiftyMinuteHour1999].

Contemporary behavioral genetics disagrees here. The prevailing story of political development splits the responsibility between genetics and environmental influence outside the home, with little credit left over for parenting [@smithBiologyIdeologyEpistemology2012]. While Lindner understands the cruelty, the anger of his patient, Anton, as a response to his father's abuse, it is possible instead that these traits were simply inherited from the father who was, by Anton's account, cruel and angry. Of course, particular political opinions are not the expressions of particular genes, but as Lindner intuits there is a strong connection between character and politics, and character is partially genetically rooted.

# 2013

> The town was an ugly swab of asphalt and concrete, a kind of hardened spittle tumoring amid forested hills. In the softly terminal stage of a relationship, I drove out to a supermarket. My carmate played a song from *The Dreaming* and pointed out a group gathered in front of a Planned Parenthood clinic. *That's Alabama for you*. There was no counterprotest; we drove on.

Not all people who believe abortion is murder show up to pro-life protests or events, not most of them. Even fewer bomb clinics. A person may come to adopt anti-abortion positions without ever acting on them. Hence, researchers distinguish cognitive radicalization, adoptions of radical beliefs, from behavioral radicalization, adoption of radical positions [cf. @neumannTroubleRadicalization2013]. It might be thought, however, that one of these flows from the others: first one must adopt political beliefs and then, when they are held with sufficient intensity, political action flows from the belief.

This view, despite its plausibility, is questionable. @munsonMakingProlifeActivists2009's study of pro-life activists concludes that what motivated them to begin their activism is not typically any strong conviction. Rather, activists are motivated by a desire to engage in a meaningful social activity. They reach a turning point in their life --- they start college and are looking to get out there and make friends --- and participation in an activist group presents itself by chance as an opportunity to meet this desire.

You do not need profound conviction to stand outside and hold up a sign. You need not even agree with the sign: maybe you just like the idea of standing in a group, holding a sign, and shouting. However, stand out long enough and the sign might convince you, either as a result of motivated reasoning (it's easier to explain your activism to yourself and others if you believe in it) or by ordinary persuasion (you're surrounded by other signs all trying to convince you). On the reverse side, agreeing with the sign, even agreeing with it passionately, may well not be enough to get a person to want to get up there and hold it themselves.

After all, there are myriad reasons one might have for refraining from participating in a political action that one agreed with. You could be busy. Standing out there with a sign might seem boring. You might doubt that it is politically effective. You might not want your friends to see you as person-that-holds-signs. You might think that the sign-carriers are assholes. Political activity is not simply an expression of political belief, but, like any activity, one that is the product of a person's beliefs and desires in a context. This is true even for the more straightforwardly expressive action of tweeting one's political opinions.

# 2014

> He lived on the intercoastal, not in one of those multi-million summer homes, but an ordinary apartment tower. Outside, moonlight glinted vaguely atop shifting crests as he described a senior thesis on *To the Lighthouse*. I had only read *The Years*, but feigned familiarity, my finger running up a tower of spines of unshelved books. It reached *The Fountainhead* and he smiled, faintly embarrassed. *I mean, yeah, but I actually think the writing is not terrible.* I nodded with an excess of sympathy. *You know I used to buy into all of that shit.*

One and the same utterance may be a declaration, a confession, a disavowal, a sympathetic gesture, a flirtation, or multiple of these at the same time. Exactly how we should describe the action depends on what it was performed for, the intention from which it sprung. As @macintyreVirtueStudyMoral2007 argues, we should understand this intention in terms of the agent's integration of the event into a self-centered narrative. Is this part of a story of political reformation? Our life, the totality of our action, is then, according to Macintyre, unified by the narratives we run through it.

The musical *Assassins* examines assassinations and assassination attempts on American presidents. Beneath the variety of assassins and motivations, the musical uncovers a yawning chasm of need for recognition, a yearning for some special historical importance, a despair at one's present deprivation and lowliness. In particular, it does not understand the assassinations as especially or universally ideologically motivated. (Giuseppe Zangara proclaims, "No left. No right. Only American. American nothing.") These assassination attempts, rather, are the culmination of a *quest for significance*. The characters tell, and tell most of all themselves, a story in which the assassination of a president is the fulfillment of a personal need. ("Someone's gotta listen.")

@kruglanskiThreePillarsRadicalization2019 understand the radicalization of Islamic terrorists in similar terms. A terrorist, in their understanding, first experiences a particularly acute psychological need for significance, for esteem, for recognition: they feel unimportant, or have their sense of importance threatened. They are offered, or conjure for themselves, a narrative, a quest for significance, in which an act of terrorism is *the* solution to this psychological need. Crucially, the terrorist must despair of any other means for addressing this needs. (In *Assassins*, the narrator uselessly claims that America offers entirely licit opportunities for achieving significance, "Haven't you heard the mailman won the lottery.")

An act as premeditated as a violent act of terror is the result of convincing oneself of a narrative in which terrorism is necessary. Typically, such conviction by a network, often by a network drawn from a terrorist group. Within the network, accepting such a terrorist narrative itself becomes a means for some measure of recognition. Even "lone-wolf" terrorist may well find a (decentralized) network pushing and rewarding one another for accepting terrorist massacre. Think about the Christchurch mosque murderer live-streaming his massacre to a forum of sycophants. *Assassins* further suggests that even political assassins with no apparent network may find company in the imagined community of past killers. Instances of "copycat terrorism" [cf. @nacosRevisitingContagionHypothesis2009] may point to a real phenomenon there.

# 2015

> I almost had the fucking project working when they scrapped it. I had spent weeks trying to integrate our partner's piece-of-shit security solution with our piece-of-shit search product, had scrutinized every syllable of their scant documentation, had learned more than I ever wanted to know about Solr's internals, and then no one even fucking wanted this in the first place. Give me a fucking break. I replied to my boss's announcement with two-weeks notice.

The kind of radicalization narrative I am interested in here are personal, cognitive radicalizations: descriptions of personal experiences which led the subject to some radical revision to their political beliefs. Here is an example.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">When I was 11 my mom would let our neighbor use water from our hose bc his utilities were shut off (he was elderly and living off of SS). Apparently this is illegal, and the cops came to shut off our water as well. It cost $400 to turn our water back on. <a href="https://t.co/jZjT6EzpHN">https://t.co/jZjT6EzpHN</a></p>&mdash; brandy (\@brandleminaj) <a href="https://twitter.com/brandleminaj/status/1290876073098518528?ref_src=twsrc%5Etfw">August 5, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

This is in response to a tweet asking "What radicalized you?" Such narratives are rhetorically appealing: they represent (cognitive) radicalization as a rational response to obvious injustice. Note, however, that the rationality of such inferences is far from assured.

In such narratives the particular injustice may receive a range of more or less radical explanation. Perhaps what this case illustrates is that the laws are written by capitalists to enforce their own interests at the expense of the working class, which issue a socialist revolution is needed to address. Perhaps though the issue is more local, confined to corruption around utilities, this jurisdiction, or utilities in this jurisdiction. Perhaps the laws themselves are fair --- we have precious little to go on as to the nature of the violation --- but they should have been waived. Perhaps the injustice is further back, concerning the neighbors poverty. Or, perhaps, if you've a Randian bent to your thinking, there is no injustice here: the neighbor was irresponsible or insufficiently hard-working, the family violated their contract with the water company, and justly had to face the agreed-upon consequences.

This last possibility was somewhat painful to type, but the basic point remains. Events do not carry their morality on their sleeves. The wrongness of a stabbing is not given to experience in the same way that its duration is. Rather it is something we have to interpret. This is even more true of politics, as our political understanding of an event supervenes not only on our moral understanding but also our broader political beliefs. As a result, such personal radicalization narratives present political radicalism not as a logical conclusion but a smuggled premise. The point being not only that the inference here is irrational, but that it is psychologically implausible: one would not come to hold radical political opinions on the basis of such experiences unless one was already primed to accept such positions.

Of course, I may well be accused of being uncharitable and perhaps a bit dense here. This is a tweet, not an intellectual autobiography. It is not attempting, and could not attempt (unless as part one of one of those dreadful Twitter chains) a complete account of radicalization, but presents merely a single, dramatic fragment of such a larger narrative. Sure, not much can be concluded from a single incident, partially described, but with more background, further incidents, and further reflection, the picture can be filled out. Indeed, when given more space than a tweet ([for instance](https://thelinknewspaper.ca/article/what-radicalized-you)), subjects will contextualize such incidents as the start of a longer intellectual journey. Such narratives, then, should not be faulted as bad arguments, given that they are not intended as such. What are they intended as?

We may still hypothesize a rhetorical purpose which is not directly an evidential one. Kenneth Burke describes rhetoric as a process of identification between the reader and rhetorical speaker. Central to any such process is the reader's coming to see the desirability of identifying with the speaker. In the case of an ordinary argument, the reader begins their identification with the reader by sharing with him certain common assumptions. The argument shows that certain conclusions follow from those assumptions, yes, but thereby that it would be good (rational, likely correct) for the reader to join in. But our temptation may not consist in any cold rationalism. Instead, the speaker may entice by the warmer virtues of clarity, of care, of compassion.

# 2016

> Trump is our new president and I am our new Cassandra, typing to those flyover Trojans through a drunk, Californian keyboards dark propositions that, to a one, will fail to come true. These yokels are Facebook friends of the second degree and so my vituperance carries with it no real consequence. I will never admit that I did not vote for Clinton; I could not drag myself out of bed to Pokemon Go to the polls.

2016, in one telling, was the years the radicals won. Unfortunately for radicals on the left, it wasn't them. Of course, that's only one telling. In another telling, 2016 was the year Russian trolls subverted American democracy. In yet another telling, 2016 was the year American democracy worked pretty much as it usually does except for some reason the Democrats nominated an historically weak candidate and the Republicans primaried a bunch of establishment schmucks against a TV star who the media could not keep their cameras off because he was good for ratings. I mean, the last telling is certainly the accurate one, but at the time it was the freaks who won my and every other mainstream Liberal heart. Did you know they came from the internet? You know, Bannon and Pepe and the rest? And here I was thinking the internet was just where we read the *New York Times*.

The definitive guide to that freak show is Angela @nagleKillAllNormies2017's *Kill All Normies*, which provides an history of the online culture war, focusing on the right, from 2012 to 2016. In the beginning was 4chan. 4chan, an anonymous internet forum, attracted users who developed a peculiar argot and a devotion to trolling, ironic humor, and shocking, transgressive posts. 4chan is an imposing place for outsiders, one of @lewisInnerRing1944's "inner rings," who have to learn a new vocabulary and hazard a harsh and confusing place. The benefit, as Lewis suggests, is to be part of the club. Naturally, that club includes Nazis, the coolest and most outré provocateurs.

4channers might be the result of techno-incubated ethnogenesis, but the Nazis predated them. As @winterOnlineHateFarRight2019 notes, far-right groups were not especially effective users of the early internet. Throughout Obama's presidency, however, far-right groups successfully grew their own fora, notably *Stormfront*, and later successfully "viralized" this community, deliberately entering other communities, communities which were not necessarily especially right wing or even especially political. Such spreading was at least in some part strategic, as suggested by "A Normie's Guide to the Alt-Right," a post on neo-Nazi website *Daily Stormer* which instructs fellow neo-Nazis on how to interact in irony-laden online fora like 4chan in such a way as to fit in with and convert the locals [@anglinNormieGuideAltRight2016]. Such conversion, however, cannot be accomplished merely by integrating oneself in a community --- most 4channers hate and ridicule most other 4channers --- one has to make being a neo-Nazi aspirational.

Hence, the neo-Nazis espousal of self-improvement. Dissatisfied and isolated in their lives, perhaps, power-users of 4chan may have be drawn to the inner circle of 4chan for some sense of community, only to discover that this community cannot and has no interest in improving the conditions that brought you there. Unless, of course, there is yet a deeper inner circle that can do this. As @elleyRebirthWestBegins2021 documents, neo-Nazis online have formulated an extension of the traditional fascist vision of the "New Man:" the reformed and radicalized ex-loser who through exercise, self-work, and, coincidentally, adoption of neo-Nazi ideas, becomes a winner, gets the girl, and destroys his (racial and political) enemies. When one takes this "iron pill" is taken, the dumbbell becomes a rhetorical device, each curl bringing the reader into greater identification with the "physically fit, handsome, sociable, successful, hygienic, happy, independent, proud, capable and competent" Nazi.

![A meme often posted to 4chan's self improvement board.](/images/radicalization/BeforeAfterSig.webp)

It is not altogether surprising that far-right online radicalization takes a similar form to other radicalization. The above image, and similar posts, function as narratives of radicalization centered around a quest for significance. They take their intended reader's psychological needs and contextualize them in a particular way --- our enemies are responsible for your loneliness, your distress --- and tell a story about how a certain course of action --- the iron-pill --- will treat that need. This is carried out in a (decentralized, online) network of other speakers reinforcing this narrative.

> **Anonymous** 08/28/16 (Sun) 06:07:38 ID IGvpantIG No. 86877726
>
> The concept of “THE” red pill is flawed. There is no single revelation. It’s not one red pill, it’s an unending course. Like antibiotics. Or, more accurately, like addictive painkillers.
> 
> --- 4chan post, quoted in @elleyRebirthWestBegins2021.


The online far-right is thus not confined to their own private compounds, but are present in every major social media site. Special attention has been paid attention to their presence on YouTube, where commenters describe a "pipeline" of connections between channels, leading from more moderate right-wing channels through to the far-right [cf. @munnAltrightPipelineIndividual2019; @ribeiroAuditingRadicalizationPathways2020]. The pipeline metaphor is unfortunate. It casts watchers of these channels as essentially passive participants in their own radicalization.

Much is made, in describing this model, of YouTube's recommendation algorithm. YouTube will recommend videos from the video a user is currently watching on the basis of its system's prediction of what the user will likely also watch. Typically, YouTube will automatically play its top recommendation after the current video is finished. It is common for users of YouTube to consume its videos in a passive way, letting them play in the background while doing something else, so it is possible for a user to watch (or, more likely, to hear) a string of increasingly right-wing videos in a sitting. You thus have a model of how YouTube could radicalize viewers: one starts watching a moderate, right-leaning channel, which, given that its interviewers intersect with that of a farther-right channel (perhaps the host has on farther-right figures as guests), leads to one watching increasingly right-wing content. Let this go on long enough and, bingo bango bongo, you've got yourself a newly minted far-right believer.

And you can find people describing themselves in this way. @rooseMakingYouTubeRadical2019 quotes one self-proclaimed ex-radical calling themselves "brainwashed" by this pipeline. Brainwashing, at least in the popular conception, describes a sort of coercive persuasion, experimented with by the Nazi Gestapo and the CIA, which consists first of putting the subject into a hypnotic, hyper-suggestible state before forcing on them new beliefs or even a new personality [@anthonyTacticalAmbiguityBrainwashing2016 219f]. Such radical, forced transformation was not found to work, but brainwashing nevertheless entered the public consciousness as CIA propaganda during the Cold War to explain Communist conversion especially among Wester soldiers in the Korean war [@anthonyTacticalAmbiguityBrainwashing2016 221]. Such an explanation plays up the threat of one's ideological enemies while casting conversion towards that enemy as both irrational and blameless.

I worry that thinking about far-right radicalization and the pipeline model suffer from the same defect as early thinking about brainwashing. If converts to your enemy are made by hook and crook rather than honest, potentially rational change of beliefs, then such conversions are in no ways evidence for your enemy's position. (In the way that sober-minded physicists' adoption of quantum mechanics is evidence in favor of quantum mechanics.) At the same time, such stories help us forgive our former friends: they weren't themselves! Anyone could have succumbed! The pipeline model of online far-right radicalization shares these features and is similarly dubious.

The pipeline model has two further major issues. The first is that the more passive we make viewers with respect to their increasing consumption of farther-right content, the less plausible it is that such consumption actually results in ideological change. If Steven Crowder is yacking away in another tab while I'm browsing pictures of cats, probably I don't even hear half of what he's saying, much less form an opinion about it. Similarly, changes in the ideology of media one is consuming need not reflect changes in one's own ideology. A viewer may follow a figure they find interesting, funny, provocative, or otherwise worth watching without signing on to that figure's politics. (You might watch Jordan Peterson lectures just for the self-help, or just for the politics with which one already agrees.) Consumption is not agreement. Moreover, shifts in consumption may reflect not a consumer's becoming more radical but finding sources more in line with their existing radical beliefs [@mungerRightWingYouTubeSupply2020]. I have not found defenses of the pipeline model which properly address these confounding factors.

# 2017

> The clothes soaked in a bathtub, were wrung out by the sink, and dried over the shower rod. There was, one supposes, a perfectly functional washer and dryer nearby in the apartment complex. But that would have required quarters and quarters required speaking to a cashier a couple hundred meters down the road. It had been all I could do to assemble the other chair for my brother's upcoming visit.

Depression is constituted not by a particular emotional experience, a feeling blue, but by a restriction on how the world can matter to us, by what Heidegger would call a mood [cf. @ratcliffe2013mood 160]. When depressed, objects lose their positive significances: the dog in the park loses its power to charm; the treat, its power to entice; the friend, their power to console. Depressed patients, correspondingly, are more averse to losses and risks than usual [@chandrasekharpammiNeuralLossAversion2015]: if nothing promises mirth, all is a sterile promontory, then best to stick to what you have, what is at least not unbearably painful. 

Such loss aversion, like many of the mechanisms of depression, is likely to be counterproductive. The depressed patient, terrified of worsening her barely tolerable situation, is likely to resist changes that will actually help her. Another such regulatory mechanism is rumination: one fixates on depressing objects causing one to feel, quelle surprise, more depressed [cf. @nolen-hoeksemaRethinkingRumination2008]. We may understand such tendencies in depressed subjects as part of the dynamics of depression: defining an attractor basin in which depression is a fixed point [@hayesDynamicSystemsTheory1998].

This mathematical model is not in itself a theory of depression, but a picture in which we can situate and unify other theories. For example, understandings of depression that highlight the role of intrinsic, individual features in explaining the condition --- cognitive styles, genetic and biochemical explanations --- might be describing the size and shape of the attractor basins, while understandings stressing the role of an individual's psycho-social conditions --- poverty, illness, the burdens of work and childcare --- point to the pushes which send an individual into the basin. We might further suggest that the experience of depression, the mood we pointed out, is tied to the central prediction of this theory: the imperviousness of depression to small, ordinary changes in environment and mental state. Depression as a mood consists in the awareness of this stasis permeating one's experience. Not only are these things lousy, but everything is lousy and nothing can change it for the better. I'll always come right back to where I started. A consequence of this picture is that a period of depression should be understood not only historically but systemically, i.e. not as a particular series of phenomena (I felt sad and then I felt sad some more) but additionally as a set of counterfactuals (were I to try not to, I would still feel sad). It therefore has a natural representation as an interactive fiction.

@quinnDepressionQuestInteractive2013's *Depression Quest* is a Twine game about having depression. You, an unnamed protagonist, struggle through a rather ordinary experience --- boring job, girlfriend, a small social group --- and have to get by. We are introduced to this background with all the grace of an encyclopedia entry. What follows is a flat series of choices. Our protagonist goes through a series of ordinary situations (invited to a birthday party, meeting a therapist, dinner with their family, work shit) and are offered a choice from a list of responses. The undepressed choices are crossed out --- a constant reminder that you cannot just "throw off" this state --- and the actual options range from modest improvements (adopt a kitten, start seeing a therapist) to wallowing in misery. Below these options, textboxes report on the intensity of your depression and the effectiveness of your therapy and medication. If you make the obviously good choices, you end the game having kept your life together and with reason for cautious optimism. If you make the obviously bad choices, your depression quickly worsens, making the obviously good choices mostly unclickable, and your life falls apart.

*Depression Quest* is an artless, earnest experience that is very insistent on passing on what it understands as the core truths about depression: depression can be managed with honesty, the right strategies, and external support, but it is also a condition that impairs one's ability to do exactly those things. There is a tension between the rhetoric of its writing --- a series of dull sentences pounding into one's head that our protagonist is not in a state to do anything helpful --- and that of its choices --- where nevertheless the helpful, productive option is only one click away. This is the result of a failing endemic to interactive fiction --- a sequence of hyperlinks is a terrible representation of the feeling of choice --- but it is a particularly embarrassing failure in this context.

With that being said, this game would have, on its own merits, faded into a quick obscurity had it not been the catalyst for, and this is saying something, one of the stupidest and longest controversies to ever embroil the internet. To tell the story as quickly as possible, Quinn is falsely accused by an ex of having slept with a journalist to obtain a favorable review of a free game. This gets picked up by an incipient harassment campaign, Gamergate, which begins bullying (up to and including death threats) Quinn and others. Gamergate ends up persisting for a couple ignoble years, petering out sometime in 2016, and has been identified with the larger activities of the online far-right, sharing certain principle players, message boards, and regressive themes.

That, at least, is the simple, unsympathetic telling of what happens. In another telling, that of @youngAlmostEverythingYou2020, the online movement comes off looking rather better. Yes, the harassment did occur, but that was by a mad fringe of an otherwise respectable group who had legitimate complaints about the gaming media and industry, which, in their view, had been captured by corruption and extreme progressive politics. Yes, there were far-right loons, but Obama voters predominated. Anyone could append '#gamergate' to their tweet or join the reddit forum; there was no structure or strategy in place.

There is a dynamics also of beliefs. @gardenforsKnowledgeFluxModeling2008 describes a rational believer as seeking belief states that maintain an equilibrium between rational pressures. An early astronomer watches the planets wandering about at night and accepts, as their contemporaries accept, a simple geocentric model. All the heavenly bodies revolve in circles around the earth. After further, more careful observation, though, the evidence and the theory just don't add up: the planets are just not moving how the model says they should howsoever one fiddles with the parameters. The astronomer is, for a moment, out of equilibrium. Rational pressures impel them to change their beliefs somehow, but how?

There is no single, obviously correct answer here. The astronomer might change their model. Perhaps they could try out this epicycle idea that certain of their colleagues have mentioned as a refinement of their geocentric model. Or maybe they picked the center of the solar system incorrectly and it's the sun not the earth. Or maybe they're just worse at this whole observation and recordkeeping business than they thought and the simple geocentric model was right all along. This is an instance of the underdetermination of theory by evidence: two astronomers looking at the same series of observations can come to different conclusions without either having or having obviously made a mistake [cf. @stanfordUnderdeterminationScientificTheory2021].

These equilibria can be more or less fragile depending on the believer, the belief, and the environment. A die-hard geocentrist astronomer can reject or explain away any apparently conflicting evidence, after each small perturbance pulled back to the central earth. The measurements might be wrong, or, if consistently wrong, the instruments broken or miscalibrated. The reports of rival astronomers might be erroneous, or, if these persist, the rivals might be lying, and, if they lie persistently enough, might be a part of a vast conspiracy. In each case the astronomer's geocentrism comes up against other commitments --- that they are a competent measurer, that their instruments are reliable, that their peers are trustworthy --- but, if they are certain enough that the sun goes round the earth, there is no reason, psychologically and perhaps even rationally speaking, that this certainty should ever fail. As a result, two believers, starting with different assumptions, may be given all of the same evidence without converging to the same belief.

While there is no guarantee of convergence, there is always the possibility of collaboration. For our evidence comes not only from our shared environment but from each other: most people's astronomical beliefs (e.g. that the sun is 93 million miles away) come not from staring through a telescope but reading, say, a Wikipedia article. We are always receiving reports from one another and have to decide whether to accept them, i.e. whether to trust their source. Trusting someone's report is at least partly a matter of assessing their competence: does this person look like they've at least seen a telescope. An inavoidable part of assessing someone's competence is checking their track record, i.e. whether they agree with me. Given a natural, rational desire to listen and speak to people one trusts, the formation of communities of like-minded believers is inevitable. A equilibrium in one's beliefs can grow to contain an entire group. And just as two people can look at the sky and come up with different pictures of the solar system, two communities can have access to the same observations and come to two stable and different pictures.

The protagonist's life in *Depression Quest* is not obviously a depressing one. Sure, their job is not great, their family imperfect, their future not promising the wealth and security of professional life, but they have things mostly together: financial security, personal interests, meaningful relationships. In short, they have the kind of life many might be reasonably envious of, and of course even more enviable lives, those of the wealthy, may be marked by depression [cf. @lutharCultureAffluencePsychological2003]. From an outside perspective, one may, as the protagonist's mother, reasonably ask what the person is so goddamn glum about. But for the depressed person there is a void at the center of the solar system --- a conviction that things are bad and only likely to get worse --- too fixed to be shifted by mere phenomena.

The same can be said about Gamergate. On the face of it, people's beliefs about Gamergate --- it is or is not a harassment campaign; it is or is not dominated by or a gateway to the farright; it is or is not centrally concerned with ethical and political issues in gaming and video game journalism --- can seem, especially to the other side, curiously recalcitrant, immune to contrary evidence. But there is no pre-theoretical evidence --- an observation only becomes evidence for or against an hypothesis when interpreted by the light of a background theory --- and one's community will filter and emphasize observations so that what one reads is already prepared to support one's existing understanding.

This is all, of course, treating everyone highly sympathetically. In reality, our beliefs are not so pristinely motivated.

# 2018

> Ash rose from the bonfire. The partymembers gathered about in green, plastic chairs. The air smelled faintly of whiskey and smoke. I gesticulated angrily. He had cited Leibniz, a man lighting only a piece of a cellar and all that. *This is not an ordinary philosophical dispute,* I insisted. *You want to believe in possible worlds, whatever, go ahead. But fucking theodicies.* Later someone pulls me aside, *Leave him alone; NA saved his life.*

Pascal's wager goes something like this. God promises you eternal bliss if you'll only pay him the compliment in believing in him. Maybe He exists, maybe He doesn't. If you believe in Him and He exists, you get the big payoff. If you believe in Him and He doesn't, well, you haven't lost much. On the other side, there's no percentage in atheism. You don't get anything but Richard Dawkin's approval, and that doesn't go for much nowadays.

Worry the first. You cannot start believing something just because it would be nice to believe it. Fair enough. But you can behave in ways likely to change your beliefs. You can start going to church. You can read every defense of Christianity you can get your hands on. You can start debating atheists on online fora. If you're self-conscious about doing these things, conversion is unlikely. But, if you forget for a moment what you are doing, it is possible that such a regime will be effective in minting a real believer.

Beliefs can be motivated by much lower stakes. In the previous section, for instance, I wanted to draw a connection between depression and radicalization, to posit depression as a significant factor disposing people towards participation in movements like the online farright and Gamergate. This motivation was a weak, aesthetic one: a paragraph there would have tightened up, unified the section. Like Pascal's aspirant Christian, I cast about for evidence in a motivated way. I could have, for instance, inserted @desboroughGamergateSocialHistory2017's account of their GamerGate-inspired suicidal gesture. Except that there does not seem to be any there there, beyond a few anecdotes. More rigorous studies however, do not uncover particularly strong links between depression and political ideology [@bernardiDepressionPoliticalPredispositions2020] or depression and radicalization [@misiakSystematicReviewRelationship2019]. 

While it is possible to slow down, to catch ourselves engaging in motivated reasoning, much of our belief formation operates according to quick rules of thumb prone to irrationality [see @kahnemanThinkingFastSlow2013a]. With this point in mind, we should be concerned not to make too much of the argumentative efforts associated with political radicalization. Radicals typically are not made by rational persuasion nor do they seek to reproduce in such a way. Politics in this way is much like religion. The arguments against religion are simple, obvious, and compelling: the world is much to shitty to have a providential designer and the diversity, nature, and distribution of religious belief obviously reflect historic contingency rather than eternal verity. These facts do little to impact religious belief, precisely because religious conviction is not the result of honest inquiry.

We might find another comparison between politics and religion in the centrality of rituals. Here I follow @collinsInteractionRitualChains2005's development of Durkheim's (via Goffman) understanding of rituals. For Collins, a ritual occurs when a group is gathered and achieve a mutual focus of attention on some object or activity and share or come to share a common emotional experience. Through rhythmic interaction, mutual attention and emotion intensify until the ritual ceases. Collins describes this emotional buildup as a collective effervescence. In a successful ritual, this shared experience should produce in the participants a certain quantity of emotional energy, by which Collins means feelings, e.g. of anger or confidence, which motivate the participant to future action, alongside a sense of group solidarity. Such feelings can be projected on symbols connected with the rituals, so that they are identified with the individual and the group. Finally, the ritual may generate a moral commitment by the individual to the group and its symbols.

Take as example confession in the Catholic church. Here we have a formal ritual of the minimum group, a confessor and a priest. The pair give mutual attention to a shared activity, the confessor's recitation of sins. We have rhythmic entrainment through mutual turn-taking, the confessor confesses and the priest probes. The shared emotional experience is one of guilt yielding to relief. The ritual has its appointed climax in the priest's recitation, "I absolve you from your sins in the name of the Father, and of the Son, and of the Holy Spirit." Afterwards, if all has gone well, the penitent and the priest alike will come out feeling renewed, confident, their attachment to the Catholic church, symbols, and morality strengthened. Given this influx of emotional energy, the participants are disposed to seek out new rituals and, in particular, to engage in similar, i.e. Catholic, rituals. In this model, then, the success of a religion, or an institution more broadly, has much to do with its capacity to control symbols and arrange circumstances associated with successful rituals and to chain these rituals together to sustain a body of believers.

Of course, rituals are not always successful, as evidenced by the fact that something like 60% of American Catholics fail to attend confession once a year [@coopermanParticipationCatholicRites2015]. Moreover, participation in a ritual is not always driven by expectation of achieving collective effervescence. As the same study shows, a majority of American Catholics admit to participating in Catholic activities to please or out of obligations towards their friends and family. Not only can ritual participation drive group identification, but so can the reverse. For Collins though there is a close, almost analytic, connection between success in a group's rituals and solidarity with, commitment to that group. Success and commitment also correlate with, with causal chains running in both directions, taking seriously the symbols, the values, and thereby the ideology of the group, becoming a true believer.

The confession ritual, as with many religious rituals, is stratified. Certain individuals, the priest, take up a more active role over certain passive others, the penitent. Just as the ritual itself may be more or less formalized, the respective positions of the participants may be more or less formalized. (The evangelical preacher is the official active participant, but at an appointed time in preaching may ask those who feel called upon to take up an active role.) Just as symmetrical rituals reinforce egalitarian group belonging, asymmetrical, "power" rituals reinforce group hierarchies. To take a dramatic example, voting in dictatorial states like North Korea becomes a power ritual. The populace is expected to attend and to vote yes for the appointed candidate, which expectations are enforced [see @marquezVarietiesElectoralExperience2014]. Such ballot-casting dramatizes the regime's power over the individual and functions as a negative ritual, aiming for not collective effervescence but collective depression, submission to the regime.

This is an extraordinary political ritual, but politics as a whole is shot through with rituals, of both formal (elections, addresses, political meetings) and informal (protests, political commentary, political conversation) varieties. Just as a religious institution operates and maintains itself in large part by coordinating religious ritual, so does a political institution coordinate political rituals. As a central example of such a ritual, consider politically inflected news. On Breitbart today, for example, I count a baker's dozen of antivaxx headlines. As one might expect, these articles frequently do not reflect what an impartial observer might consider newsworthy, but, I suggest, serve a ritual opportunity: an opportunity to "get mad" in a way that is energizing and reinforces group solidarity, symbols, and values.

Collins insists on the social character of ritual and is likely correct to assert that the intimacy of a ritual plays a key part in its power. However, he makes room for "internal rituals," which may be performed when one lacks opportunities for engaging in an adequate external ritual. For example, prayer may restore or maintain one's conviction when the churches are closed. One's position in an internal ritual may be passive, as when one recites a prayer from a prayer book, or active, as when one "has a conversation with God." Moreover, such an internal ritual may be preparation for an external ritual, as when one plans what one is to confess or the priest practices their part in the Eucharist.

We might point out that asynchronous, distant communication (letter writing, blogging, tweeting) functions as a pair of internal rituals. In one, the author engages in a certain imaginative ritual, taking the active part in conducting an imagined speech to an imagined readership. The reader of the product of this ritual takes a passive part in a dual internal ritual. Thus, for instance, the production and distribution of radicalization narratives is a ritual in this sense: the author seeks "collective" effervescence in the activity of relaying a symbolically charged narrative. In a radicalization narrative, the shared mood and symbols are politically inflected.

(The situation is somewhat more complex on a platform like Twitter. For the point of publishing there is not only to garner attention but also to prompt replies. This may in some cases, when the cadence of replies is fast enough, permit the Twitter thread to function as a conversation. In the usual case, given the asynchronous nature of Twitter and other social media, the Twitter thread functions as a ritual chain, wherein one continually consumes and contributes to the conversation as a virtual, growing object.)

# 2019

> The city has become crossing planes of graffiti. Businesses have covered their windows with plywood and the plywood has been covered with spraypainted slogans. I am perched anxiously on the porch, not thinking of what is up there (weeks of grading piling up and festering), or what is out there.

It is well enough time to give some order to these stray remarks. When it comes to radicalization there are a number of strands to tease apart. We have already seen a distinction between cognitive and behavioral radicalization. Further, we can imagine toy theories that center each account of radicalization. On a *cognitive-first* model of radicalization, radicalization is essentially a process of belief change. The radicalized is someone who takes on a series of "red pills" offered by the already radical. Such a model can make room for the affective, the motivational in terms of the motivation of inquiry: a certain degree of dissatisfaction with orthodoxy may needs precede going looking for such pills and their peddlers. But the inquiry is ordinary research, if carried out under conditions of human frailty, and, crucially, a change of beliefs precedes radical behavior. You have to be convinced that throwing bombs is necessary before being moved to throw bombs.

Except, we noticed, this does not exactly fit the mold of observed radicalizations. The pro-life activist often starts at a protest with only a vague antipathy towards abortion. Neo-Nazi groups will bring prospective recruits as participants in hate crime before they have imbibed much by way of Nazi ideology. Group bonding and behavioral radicalization come in advanced of radical beliefs. We may refine this *behavior-first* model by noticing that these examples are *radicalizing rituals*: interaction rituals, in Collins' sense, par excellence. A protest, for example, involves a high degree of rhythmic entrainment (the group marching in step, shouting slogans in a call-and-response) centered around a set of symbols.

It has been observed that terrorist groups have a *free rider* problem. Frequently enough, they exist among populations where there is a substantial degree of support for terrorist activities and, even more so, for the aspirations of such groups. Actual being a terrorist, however, encounters a collective action problem where a group can agree that one of them should commit terrorist violence, but no member of the group wants to be the one to do it. After all, you'll probably die. This is the kind of problem that drawing lots was invented for and such a procedure, or a group leader who can assign roles, can solve the problem.

Note that this problem is extremely general and often much less dramatic. All kinds of unsexy labor suffers from free rider problems. The protest requires not just protesters but members of the group to coordinate rides and provide water. Your cult members can't spend all of their time in dynamic meditation; someone has to clean the toilets. That is, an institution has to engage in both goal-directed and reproductive activity. There is overlap here. Terrorist violence may be both goal-directed and reproductive (insofar as it boosts the organization's reputation and inspires new recruits). A key point here is, first, that radicalization efforts are a key piece of reproductive labor for radical groups, and, correspondingly, that we therefore have to understand radicalization not just from the perspective of the radicalized individual but the group which needs such radicalizations to maintain itself.

One fact which an excessively cognitive point of view threatens to obscure is that radicalism suffers a tendency to decay. We tend to think of beliefs as more-or-less static. Once I have reached an equilibrium, my beliefs tomorrow will be the same as those today, unless something occurs to upset the mix. Radical behavior, however, has an inherent tendency to decay. A society which has any hopes of lasting is one in which normal, i.e. unradical, behavior is the path of least resistance for almost all its members. To take a novelistic example, Nick Hornby's *How To Be Good* depicts a bourgeois marriage, temporarily beset by a bout of radical moralism, reforming about the typical, the normal. Intentional communities have a rather poor track record. Reproductive labor in a radical organization is therefore of crucial importance. The ship is always sinking, will always need bailing out.

Thus we can observe that extant radicalization models, like the staircase model, incorporate both *stasis* (people tend to stay around the same stair) and *movement* (some are radicalized, others deradicalize or simply drop out). We should understand stasis, however, not as an absence of pressures but again as an equilibrium, a balancing of pressures. A successful, in the sense of reproductive fitness, radical group is one which not only is capable of the production of such forces but which incentivizes the newly-inducted to become proselytizers. That is, a successful group is one in which reproduction is sexy.

I want to suggest that the success of online radicalization owes in large part to this very sexiness. Online radicalization shares the advantage of traditionally successful radical groups of the central radicalizing act being a powerful ritual both for prospective and established group members. The long-time protesters can feel proud and confident as the leaders of the protest while the newcomers can experience a rush of emotional energy. Whereas online the radicalization narrative and the shitpost alike serve a similar purpose: the poster gets to feel confident and in control and the reader can feel energized, scandalized, outraged. From such things are group solidarity and symbols born. And what the online forum lacks in intensity it can make up in ease.

This poses an opportunity for the radicals: accelerated reproduction via ideological pornography. But for the genuinely ideologically driven radical, it also poses a problem. Online groups have the same free rider problems as any other group, but they lack much of the infrastructure that allows in-person groups to overcome that problem. The online group may not have the same issue of their being unsexy reproductive labor. The tubes of the internet do not need scrubbing in the same way as the compound toilets. But they face the same problem of boring-old goal-directed labor. At some point, if you want to get anything done, you have to stop posting and go outside. If posting is the main draw and outside is a drag, it's less clear how this is going to get done.

I have not yet come across all that much data covering these dynamics. I suspect that not only are these primarily online radical movements ineffective at achieving their goals, but would further guess that they sap membership from more active offline communities. The main claim here is that we should interpret what, on the face, looks like a principally ideological, propagandistic phenomenon --- the radicalization narrative and the wider genre of radical posting --- as primarily instead a central example of radical and radicalizing behavior. Our portrait of the to-be radical, then, should include not or not only a litany of intellectual vices --- a curious combination of distrust and gullibility --- but (also) an affective openness towards involvement.

# 2020

> After getting off the phone, I find myself caught by a rare bout of nostalgia. I am not looking for anything, and yet, stumbling across the image, I decided that I had found it. Vague abstract shapes floated around one another, casting shadows from an undisclosed light source, a title imposed itself faintly on the bottom. I had never got around to editing the draft; it could never live up to its cover.

In another Sondheim musical, *Into the Woods*, the fairy-tale characters are caught in the aftermath of fairy-tale moments, marrying a prince, plundering from a kingdom in the sky. In the second act, when the unintended consequences of the character's bliss come back with a vengeance, the baker's wife wanders away from the group. In the titular woods, she stumbles into one of the princes and engages in the briefest of romances. It is an odd encounter, confirming the prince's well-established infidelity and challenging our understanding of her virtue. Tidying up, she reflects on this tryst in one of the musical's best numbers.

The piece, "Moments in the Woods," opens with the baker's wife caught between a bewildered appreciation of the moment --- "Was that him? Was it me? Did a prince really kiss me?" --- and objection to it as a dereliction of duties --- "Back to life. Back to sense." At a remove, she is likewise caught between two resolutions of this conflict, one in which such energizing ruptures have a place in life --- "That's what woods are for, for those moments in the woods." --- and one on which they threaten to destabilize a proper existence --- "And to get what you wish only just for a moment, these are dangerous woods."

Of present concern are the related thoughts, first, that such moments cannot amount to a life and, second, that one is not oneself in such moments. 

>| Oh, if life were made of moments,
>| Even now and then a bad one!
>| But if life were only moments,
>| Then you'd never know you had one.

And

>| Was that him?
>| Yes, it was.
>| Was that me?
>| No, it wasn't.
>| Just a trick of the woods.

We might relate these thoughts in the following way. An experience like this is overwhelming. Users of certain psychedelics report that their trips involve ego death. Their experiences are so extreme, so involving that the self seems to be annihilated; one has no sense of things happening *to them*, but simply to things *happening*. What is going on here is something milder, not a case of the ego being annihilated but rather its being suppressed. One simply lacks the bandwith to attend to future concerns, to worry too much about what one is doing and its consequences, and similarly one cannot contextualize one's actions in terms of the past. It simply does not make sense that a devoted wife and husband should give herself so easily over to the prince's advances. Insofar as having a life, as something more than continuing to be alive, requires such fundamentally narrative connections, a life cannot survive an abundance of such experiences, even if a human organism clearly can.

We may, in passing, remark upon the fit with Collin's theory of rituals. On the one hand, they square quite well with his thought on the power of the situation. At least in the well-functioning ritual, the actions of the participants are in large part determined by the needs of the ritual itself as something distinct from those of the participants themselves. The riot can produce behavior completely out of character of the rioters. It is in the moment between rituals that Collins finds a place for individual agency and participation. Even here, the draw of the ritual is key; Collins, along with the economist, understands the agent as a preference-maximizer, but goes further to understand a person's core preference as being towards the emotional energy supplied by rituals.

If people are fundamentally emotional-energy maximizers, the baker's wife (partial) aversion towards the moment may strike us as puzzling. Certainly there are those who resist the siren's call of the mob, and I count myself among their numbers, howsoever energizing mob participation might be. Plausibly, though, the traditional parameters of rational choice theory may be tweaked to fit the datum. What is being maximized is not moment-to-moment emotional energy, but long-term, expected energy. Individuals can differ on how long term their thinking is (their rate of time discounting) and their aversion to (or even preference for) risk. Someone invested in long-term and relatively riskless payoff may prefer the quiet, domestic life to the uncertain, extreme-filled life of adventure.

Such a rationalistic gloss does lose the emotional texture of the baker's wife's deliberation. It also misses its personal, psychological dimension. Her uncertainty concerns not centrally the vagaries of time- and risk-preference but dueling pictures of what life itself amounts to: one of life as an extended, coherent narrative, and one of life as a fragmented series of episodes. We have already seen the claim that the continuity of a person's life consists in its coherence to a larger narrative; such a descriptive claim is often paired with a corresponding normative claim that it is good for a person's life to have such a narrative unity. Note, however, that the descriptive claim is not obviously true of every person; some self-report even the undramatic portions of their lives as consisting of lower-case-'m' moments, as episodes rather than a continuous narrative. We can find a fictional example in Eleanor from Woolf's *The Years*.

> My life, she said to herself. That was odd, it was the second time that evening that somebody had talked about her life. And I haven't got one, she thought. Oughtn't a life to be something you could handle and produce?---a life of seventy odd years. But I've only the present moment, she thought. Here she was alive, now, listening to the fox-trot.

That there are people who do not understand their lives in terms of narratives, whose self-understanding is formless, episodic, fragmented, and that this is no bad thing is defended by @strawsonNarrativity2004. Sure, certain may well engage in and be very engaged by the project of casting themselves as the hero of some grand quest. They may even find some value in these things. But there are dangers here --- generating a tidy narrative may require significant distortion or even falsification of our histories --- and prospects too to dispensing with such a questing for narrative.

Nevertheless, there is something melancholy in Eleanor's reflections. Even those who do not naturally understand their life in capital-'N' Narrative terms may, with a certain wistfulness, wish that they could fit themselves into some tidy story. Such it is with the baker's wife. Her valuing of hearth and home, husband and child, is not merely a cautious desire for certain if modest happiness, but a desire for a life that makes sense, that fits a given pattern. Abandonements of such projects or sudden, unexpected losses, even sudden, unexpected windfalls (to be kissed by a prince!), threaten not merely our aversion to sunk costs but a natural, if not universal, desire for an ordered existence. A chaotic death, for instance the baker's wife's own, strikes us as worse than if one's affairs were in order, even if it is not marked by any greater suffering. In the other direction, a possibility which promises a story-book structure to our lives is thereby all the more tempting.

This is an offer that comes with political radicalization, and which exists over and above all the usual inducements: money, status, recognition, acceptance, power, righteousness. Identifying oneself with a political narrative offers to solve the great narrative problem of death. On the one hand, we may finish our quest before we die and have to live with the rather embarrassing happily-ever-after. More likely, but no less distressing, we die having only some progress to show. Understanding one's life as that of a radical, an activist, an advanced guard of some great and distant utopia, provides a way out of this bind. There is no earthly chance of actually living to see the promised land. But nor does one's own death preclude its arrival --- others are there to take up the good fight. We might expect, then, greater attraction towards and loyalty to political radicalism among those attracted to a narrative understanding of themselves. (Again, I have not come across testing of this, admittedly vague, speculation.)

On the other side, we might expect the distortions that tend to afflict our self-starring life narratives also to plague our self-starring political narratives. We noted a tendency towards excessive patness in radicalization narratives. Ambiguities, alternative accounts, confounding factors, all have been exorcised from the radicalizing event before it is put on display. As they must, if it is to work its magic. But this I think need not be merely a rhetorical effect and certainly need not be insincere. Rather in the same way that our memory and our attention come to our aid in constructing a self-centered narrative --- if we want to describe how our mother inspired our recent achievement, every encouraging word from her lips will spring to mind; questioned by a therapist, every cruel remark takes its place --- we can practice a strategic (though innocent) blindness towards the conditions of our own radicalization. Indeed, we have little choice but to do so.

# 2021

> With her practiced eye, she had spotted the offensive slogans right away, though the light yellow paint and offensive calligraphy made the messages unreadable to the passing cars. In the evening, when we went to make our emendations, the glare from the streetlights merged the consonants into a scrawl. In a nearly identical paint and differently illegible script, the other two crossed out, revised, and commented on the offending graffiti. We walked into the suburbs. In a month, it had all been cleaned up anyways.

At the end of *Into the Woods*, the surviving main cast band together and kill the giant. The act is of terrible significance and significant moral ambiguity: the giant's revenge quest came with significant justification; one of the band had killed her husband. The giant's death marks the coalescing of their new community and it forms the community's founding myth. The play closes with the baker telling his son the story of the play; it had begun with the narrator (who shares an actor with the baker's father) telling the audience the same story, in a different register. The baker's father dies when his story comes to an end, when the family curse is lifted. The narrator dies later, sacrificed to the giant. The play, then, functions as the example of the death and rebirth of a narrative.

In *Assassins*, two narratives are at war. There is the story of the assassins themselves ("Another National Anthem") in which  their projects, from grand, moralistic designs to personal ends, are essentially at odds, incompatible with the functioning of American society. Either they or the system has to go; and since you can't dismantle a system all by your lonesome, you do the next best thing. You kill the president. This mad striving, as the play makes clear, has something in common with the mainstream, American story: "Everyone has a right to their dream." But the mainstream story is one on which those dreams, at least when they are not mad, can be achieved in good-old American fashion, by "working your way to the front of the line."

These narratives and their conflict seems hopelessly eternal. As the meritocratic American story is cruelly naive, it inevitably produces discontents. As it is hegemonic, these discontents can think to engage only in weighty symbolic, but ultimately counterproductive, acts of regicide. Such acts do nothing to win the sympathy of the larger public or to advance the assassin's aims. The play closes by forgetting for a moment this systemic understanding and focusing on a personal understanding of these assassinations. Lee Harvey Oswald is visited by the spirits of these assassins, who beseech him to carry out the assassination of John F. Kennedy as a way of giving their own assassinations meaning. The following number, "Something Just Broke," makes clear that to the American public, by contrast, the shooting of JFK was a profound but profoundly meaningless event. The fact that two of the assassins addressing Oswald postdated him confirms this meaninglessness.

It is no great surprise that Sondheim, given his age, has recently died. It is some small coincidence that the news came just as I was wrapping (?) this essay (?) up (?), which features two of his musicals. (Given its length and the sluggishness of my drafting, I can hardly be accused of capitalizing on his passing.) Sondheim managed works of considerable accessibility which rarely yielded to simple-mindedness. I fear, by contrast, that this piece might rather reveal me as an obscurantist simpleton. If I have any defense, it is that the topic, like our own lives, does not yield to clean, comprehensive treatment, but only partially, in contrary flashes of light.

# References